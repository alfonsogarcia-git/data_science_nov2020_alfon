{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "En el anterior enlace, tenéis un ejemplo sobre cómo, a partir de tweets con un label específico (un sentimiento, positivo o negativo): \n",
    "\n",
    "1. Genera un conjunto de entrenamiento. El conjunto de entrenamiento es formado a partir de tweets completos pasados a un array con un tamaño específico.\n",
    "2. Ese array (X_train de tamaño N) tiene un label que representa el sentimiento (y_train)\n",
    "3. Como todas las frases tienen un tamaño N, la entrada de la red neuronal será de tamaño N y la salida de la red será de tamaño 2 usando activación softmax(porque hay dos clases).\n",
    "\n",
    "Se pide: \n",
    "\n",
    "- Realizar un clasificador de reviews para el dataset de IMDB de la carpeta data_exercise/\n",
    "\n",
    "**Cuando usa la importación \"keras.x\", reemplázalo por \"tensorflow.keras.x\"**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Complex is better than complicated.\n",
    "\n",
    "Flat is better than nested.\n",
    "\n",
    "- Tokenized: \n",
    "\n",
    "[['complex', 'is', 'better', 'than', 'complicated', 'flat', 'is', 'better', 'than', 'nested']]\n",
    "\n",
    "- In a lookup dictionary: \n",
    "\n",
    "\n",
    "  {'complex': 0,\n",
    "  'is': 1,\n",
    "  'better': 2,\n",
    "  'than': 3,\n",
    "  'complicated': 4,\n",
    "  'flat': 5,\n",
    "  'nested': 6}\n",
    "\n",
    "- In one-hot encoding:\n",
    "\n",
    "[\n",
    "  [1, 0, 0, 0, 0, 0, 0], #complex\n",
    "\n",
    "  [0, 1, 0, 0, 0, 0, 0], #is\n",
    "\n",
    "  [0, 0, 1, 0, 0, 0, 0], #better\n",
    "\n",
    "  [0, 0, 0, 1, 0, 0, 0], #than\n",
    "  \n",
    "  [0, 0, 0, 0, 1, 0, 0], #complicated\n",
    "]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Ejercicio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Se pide: \n",
    "\n",
    "- Realizar un clasificador de reviews para el dataset de IMDB de la carpeta data_exercise/\n",
    "\n",
    "**Cuando usa la importación \"keras.x\", reemplázalo por \"tensorflow.keras.x\"**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = pd.read_csv('data_exercise/IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y and encoding y to numbers\n",
    "le = LabelEncoder()\n",
    "X_train = np.array(df['review'])\n",
    "y_train = np.array(df['sentiment'])\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000,)\n(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 3000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "# feed our training data to the Tokenizer\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "# Saving dictionary to json file so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # kpt.text_to_word_sequence receives a sentence and stores each word separately in a list\n",
    "    # This returns a list of the numerical values of the dictionary of each word in the text\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for words in X_train:\n",
    "    wordIndices = convert_text_to_index_array(words)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all reviews converted to index arrays, we'll switch the list to array\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "X_train = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "\n",
    "# treat the labels as categories\n",
    "y_train = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 512)               1536512   \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               131328    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 514       \n=================================================================\nTotal params: 1,668,354\nTrainable params: 1,668,354\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.3944 - accuracy: 0.8150 - val_loss: 0.2690 - val_accuracy: 0.8832\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.2477 - accuracy: 0.8978 - val_loss: 0.2803 - val_accuracy: 0.8810\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.1959 - accuracy: 0.9179 - val_loss: 0.2997 - val_accuracy: 0.8810\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.1214 - accuracy: 0.9490 - val_loss: 0.3560 - val_accuracy: 0.8798\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 0.0690 - accuracy: 0.9710 - val_loss: 0.3961 - val_accuracy: 0.8828\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc51f1033d0>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_json = model.to_json()\n",
    "with open('model_IMDB.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model_IMDB.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the Saved model\n",
    "json_file = open('model_IMDB.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model_IMDB.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence: good movie\n",
      "positive sentiment; 92.949563% confidence\n",
      "Sentence: bad movie\n",
      "negative sentiment; 99.963331% confidence\n",
      "Sentence: loved it\n",
      "positive sentiment; 99.962723% confidence\n",
      "Sentence: hated it\n",
      "negative sentiment; 99.296749% confidence\n",
      "Sentence: high expectations\n",
      "negative sentiment; 52.942806% confidence\n",
      "Sentence: great\n",
      "positive sentiment; 99.918979% confidence\n",
      "Sentence: positive review\n",
      "negative sentiment; 94.821256% confidence\n"
     ]
    }
   ],
   "source": [
    "# Now running the model\n",
    "labels = ['negative', 'positive']\n",
    "while 1:\n",
    "    evalSentence = input('Input a sentence to be evaluated, or Enter to quit: ')\n",
    "\n",
    "    if len(evalSentence) == 0:\n",
    "        break\n",
    "\n",
    "    # format your input for the neural net, function defined above\n",
    "    testArr = convert_text_to_index_array(evalSentence)\n",
    "    inp = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "\n",
    "    # predict which bucket your input belongs in\n",
    "    pred = model.predict(inp)\n",
    "    print('Sentence:', evalSentence)\n",
    "    print(\"%s sentiment; %f%% confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}